{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 初始化wandb项目\n",
    "wandb.init(project=\"VAE2\")\n",
    "# pytorch minst数据集\n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean,), (std,)) \n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置主要参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 512\n",
    "kernel_size = 3\n",
    "filters     = 16\n",
    "epochs      = 30\n",
    "latent_dim  = 2   ## 隐变量取2维只是为了方便后面画图，适当提高可以提高生成质量，比如提高到8\n",
    "device      = 0   ## 选取gpu，这里选择了第一个gpu\n",
    "\n",
    "image_size  = train_dataset[0][0].shape[1] ## 1 * 28 * 28\n",
    "features    = 2*filters*(image_size//4)**2 ## 两层卷积后的特征向量长度\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder, Decoder, 和 loss 定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder, Decoder也可以不用卷积，只用全连接层\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=filters, kernel_size=kernel_size, stride=2, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=filters, out_channels=filters*2, kernel_size=kernel_size, stride=2, padding=1)\n",
    "        self.fc = torch.nn.Linear(in_features=features, out_features=filters)\n",
    "        \n",
    "        self.mean = torch.nn.Linear(in_features=filters, out_features=latent_dim)\n",
    "        self.varlog = torch.nn.Linear(in_features=filters, out_features=latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        mean = self.mean(x)\n",
    "        varlog = self.varlog(x)\n",
    "        return mean, varlog\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc = torch.nn.Linear(in_features=latent_dim, out_features=features)\n",
    "        self.conv2 = torch.nn.ConvTranspose2d(in_channels=filters*2, out_channels=filters, kernel_size=kernel_size, stride=2, padding=1, output_padding=1)\n",
    "        self.conv1 = torch.nn.ConvTranspose2d(in_channels=filters, out_channels=1, kernel_size=kernel_size, stride=2, padding=1, output_padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), filters*2, image_size//4, image_size//4)\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = torch.nn.functional.sigmoid(self.conv1(x))\n",
    "        return x\n",
    "    \n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "    \n",
    "class VAEloss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAEloss, self).__init__()\n",
    "    \n",
    "    def forward(self, x, x_recon, mu, logvar):\n",
    "        ## BCE loss\n",
    "        recon_loss = torch.nn.functional.binary_cross_entropy(x_recon, x,reduction='sum', size_average=False)\n",
    "        ## MSE loss\n",
    "        # recon_loss = F.mse_loss(x_recon.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
    "        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) ## batch_size, 2\n",
    "        return recon_loss/ batch_size, kl_div / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE()\n",
    "vaeloss = VAEloss()\n",
    "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "vae.to(device)\n",
    "vae.train()\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    for i, (x,y) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        \n",
    "        x_recon, mu, logvar = vae(x)\n",
    "        recon_loss, kl_div = vaeloss(x, x_recon, mu, logvar)\n",
    "        loss = recon_loss + kl_div\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\"iter\": i, \"reconstruction_loss\": recon_loss.item(), \"kl_divergence\": kl_div.item()}, commit=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vae.eval()\n",
    "for x_test, y_test in test_loader:\n",
    "    x_test = x_test.to(device)\n",
    "    x_recon, mu, logvar = vae(x_test)\n",
    "    break\n",
    "\n",
    "original_images = x_test\n",
    "generated_images = x_recon\n",
    "\n",
    "comparison_grid = torch.cat((original_images, generated_images), dim=2)\n",
    "grid = make_grid(comparison_grid, nrow=40, padding=2).cpu()*std + mean\n",
    "\n",
    "# 展示结果，第一行是原图，第二行是生成图，以此类推\n",
    "torchvision.transforms.ToPILImage()(grid).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 展示每个数字类别与latent向量的关系(当latent=2时)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "for x_test, y_test in test_loader:\n",
    "    x_test = x_test.to(device)\n",
    "    mu, logvar = vae.encoder(x_test)\n",
    "    z = vae.reparameterize(mu, logvar).cpu().detach().numpy()\n",
    "    break\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(z[:, 0], z[:, 1], c=y_test)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch-Lightning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
